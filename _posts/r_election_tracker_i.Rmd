---
title: "An R project to learn GIS, ggplot, web scraping, data management, web deployment, and data analysis"
subtitle: "Part 1: R as a GIS"
author: "John Ray"
date: "November 14, 2017"
layout: post

output: html_document
  #md_document:
  #  variant: markdown_github
---

### Introduction to the project

I've been thinking of ways to make a full data course out of one big R project, and the recent [special election in Virginia](https://ballotpedia.org/Virginia_state_legislative_special_elections,_2017) provided just that way. On the day of the election I set up a [dedicated page](https://johnlray.github.io/2017/11/07/election_tracker.html) to gather, format, and plot the results of the election as vote tallies rolled in from the various voting sites across the state. The final version of this project required running one `R` script and committing the subsequent changes to git to update a large dataset gathered from open-source web resources, store and parse it efficiently, generate plots from that data, and then post those plots to the web.

For this project, we will be walking through how to create such a schema. I have chosen to break it down into a few smaller parts so that each unit is a discrete learning lesson on one useful topic. Those topics are:

* Using R as a GIS
* The use of the grammar of graphics with the `ggplot2` library
* Acquiring unruly data from a public resource (webscraping)
* Formatting unruly data
* Deploying data visualizations and analysis to the web
* Conducting statistical analysis
* Putting together the final product

Thus, this post is the first in a series that will have six discrete lessons, and then a seventh to put it all together. My goal with seven lessons is to have a more or less finished product that can take the user from being a novice to being very good with `R` in one week's time. Specifically, I attempt to focus on certain important `R` skills that are common in practice but not taught in many introductory data science courses. Finally, the order of events may seem somewhat strange - ggplot before data storage? - but this series revolves around a real-life project I worked on and, if I could do it over again, this is the order in which I would have assembled the project.

To tie each lesson into the final product, here is how we are going to fit each of these lessons together:

* First, we are going to visualize the geography whose elections we want to track - in this case, Virginia in the 2017 statewide election
* Then, we are going to tinker with our visualization so that it displays our data in an appealing and informative fashion
* We will get some real data from the Virginia Department of Elections to add to our visualizations...
* ...then we will learn how to store that data and format it so its easy to work with
* With our visualizations complete, we will learn how to deploy them to the web using Git and my preferred blogging venue, which is .io with a Jekyll framework
* While this is not a statistical methods project, but a programming project, we will talk through some simple statistical analysis you can conduct in real-ish time, specifically the difference-in-means test
* Finally, we will discuss the full workflow to think about how to optimize our production pipeline

I proceed on the following assumptions:

1. You have `R` and `RStudio` installed on your machine.
2. You are conversant in basic `R` syntax, about what one would know by the end of the excellent [Quick-R tutorials](https://www.statmethods.net/).
3. You are sufficiently conversant in the general political context such that I can say something like "state legislative district" and you know approximately what I'm talking about. Specific knowledge of the Virginia context is not assumed.

That's about it. Let's get started!

### R as a GIS

For this project, we will be plotting legislative districts in the US state of Virginia. Here, we will retrieve spatial data representing these districts from a public resource, load that data into `R`, and plot that data within our RStudio session.

Geographic Information Systems (GIS) is a term that refers generically to any system or program that helps humans interpret and analyze geographic data. Unfortunately, `R` was not designed with GIS in mind. As a result, there are many libraries and approaches to using `R` as a GIS. I present the easiest and most popular way, under the suspicion that the other ways probably won't be updated so much in the future.

This is more than just idle quibbling on my part because the steps needed to make your machine's `R` session into a GIS. Depending on your machine's operating system, `R` will interact with geographic data in a slightly different way. By now, this mostly only manifests in the types of arguments common `R` libraries' functions will take when loading geographic data into your `R` session. Knowing this now diminishes the programming challenge to a mere triviality, but I did not know that loading spatial data into `R` could involve some very fiddly arguments when I was just getting started, and it frustrated me for a long time.

While the quality of American Federalism is such that there is [some disagreement over who lives in what state legislative district](https://wtop.com/virginia/2017/11/147-voted-wrong-house-delegates-race-va/), the US Census maintains the most up-to-date spatial data on legislative districts in the United States [here](https://www.census.gov/geo/maps-data/data/cbf/cbf_sld.html). For this project, we will be using the spatial data representing the lower house, the Virginia House of Delegates, which lives [here](http://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_51_sldu_500k.zip). While I typically prefer the fairly friendly point-and-click interface of the Census website to get my data, you can replicate this process by running the following code.

```{r download, eval = F}
download.file('http://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_51_sldl_500k.zip', '~/Downloads/cb_2016_51_sldl_500k.zip')
unzip('~/Downloads/cb_2016_51_sldl_500k.zip', exdir = '~/Downloads/cb_2016_51_sldl_500k')
```

The Census shapefile naming convention may look like gobbledygoop, but it tells us everything we need to know: We have downloaded a **c**artographic **b**oundary from **2016** covering Virginia's (state FIPS code **51**) **s**tate **l**egislative **d**istricts (**l**ower) at a **500k** resolution, which is fairly coarse by Census standards (i.e., the file will be a manageably small size!). Voila **cb_2016_51_sldl_500k**.

Note that there is some redundancy in my code because I do not use the `setwd()` convention. Also note that I download the spatial data to my machine's "Downloads" folder. Ultimately, you'll want to store this data in a dedicated project folder.

If this is your first time working with a shapefile, you'll notice you've downloaded a .zip file containing not just one but several files, all with the same name but a different file extension. As with the [Boschian Hellscape](https://upload.wikimedia.org/wikipedia/commons/1/1d/Bosch_-_Haywain_Triptych.jpg) that is LaTeX's file structure, shapefiles are composed of many disparate pieces. Census data is of very high quality and so contains constituent parts many shapefile providers do not bother to include, only a few of which are essential. An in-depth knowledge of this system is not necessary, but while we're here, I will hastily review what each file in the `cb_2016_51_sldl_500k` directory contains.

* **.cpg** contains the character set to use when rendering data contained in the shapefile.

```{r, warning = FALSE}
readLines('~/Downloads/cb_2016_51_sldl_500k/cb_2016_51_sldl_500k.cpg')
```

* **.dbf** contains the tabular data associated with the shapefile.

```{r, warning = FALSE}
library(dplyr, quietly = T, warn.conflicts = F)
library(foreign)

read.dbf('~/Downloads/cb_2016_51_sldl_500k/cb_2016_51_sldl_500k.dbf') %>% head()
```

* **.prj** contains the projection method used to generate the vectors comprising the map we will plot. The Census uses NAD83, while Google and other common providers of spatial data use WGS84 or a similar system.

```{r, warning = FALSE}
readLines('~/Downloads/cb_2016_51_sldl_500k/cb_2016_51_sldl_500k.prj')
```

* **.shp** is generally regarded as "the shapefile" itself, as it contains all the vector data a GIS would need to draw the map.

* **.xml** contains metadata on the shapefile itself, rather than on the political boundaries the shapefile describes. The .xml metadata is used to populate the Census site describing the shapefile's contents to users, and describes the contents of each of the fields in the shapefile's `.dbf`.

* **.shx** is a compiled **.shp**, which kind of makes you wonder what **.shp** is still doing here. What a world!

Note that most GISs need only a .shp (lines) and .dbf (data) to draw an informative spatial plot. The .prj is helpful for telling your GIS what shape you assume the globe to be before drawing on it, but it is not strictly necessary. Further note that each of the .xml files contained in this shapefile are for metadata not actually used by your GIS, but for convenient storage, retrieval, and identification of the shapefile itself.

If you feel like it, open the .dbf file associated with this Shapefile using a spreadsheet programs of your choice (all the most common spreadsheet programs should be able to open a .dbf file, subject to a few import options to be set by the user upon opening). That file contains the data that will show up in the `@data` slot of the shapefile object once we load it into `R`. Shapefile .dbfs have the curious property that, once encoded as spatial data, the column names in the .dbf contain descriptive information on the contents of the column. For example, the column named containing a comma and then a 'C' contain character strings, and have a number following the 'C' setting the maximum number of characters an individual row may contain. Others have an 'N' or 'I' for numeric or integer information, followed by a number describing how many decimal places a row may contain.

One may reflect on the fact this means we can easily break a shapefile by fiddling with it in a spreadsheet program. If I were to violate the maximum number of characters permitted in a row, the file would cease to save properly. Additionally, if I were to add a new row of data, that data would exist in the .dbf but would not have any vector data associated with it, and thus would not plot properly once loaded into a GIS.

**Thus, it is generally unwise to fiddle with a shapefile in anything other than a GIS.** Editing data manually in a spreadsheet is a convenient (and, frankly, underrated) means of making quick changes, but is ill-advised given the peculiar nature of shapefiles. GIS programs usually constrain data edits to avoid creating problems. `R` does not, but code can be rewritten and undone seemlessly enough that we take on less risk by using `R` to edit data rather than a spreadsheet program. Lets close the spreadsheet and count on not opening it again for this unit.

Instead, lets start plotting. We'll start by reading the shapefile into `R`. To do so we will use the `readOGR()` function from the `rgdal` library, which you should install by running `install.packages('rgdal')`. We will point the function to the location of the shapefile. Your machine uses its own particular set of drivers to interact with shapefiles, and so it is possible that the code I run (on a Mac running El Capitan 10.11.6) to load the file will have to be modified slightly for your machine. Specifically, you may have to fiddle with the filepaths: Some operating systems support tilde expansion while others do not, and Windows machines typically use a different file structure than Mac machines.

```{r loadshp}
library(rgdal, quietly = T, warn.conflicts = F)

shp = readOGR('/Users/johnray/Downloads/cb_2016_51_sldl_500k/', layer = 'cb_2016_51_sldl_500k')
```

Note that upon loading the shapefile, the output from `readOGR()` tells me that my operating system is running the ESRI Shpaefile driver to load the shapefile, which may be different from the driver installed on your machine. Some Google-fu will be necessary to check your own machine's shapefile driver.

`R` represents the various pieces of a shapefile as slots of one object. Just as variables in a dataframe are accessed using the `$` operator, object slots are accessed using the `@` operator. If you chose to name your shapefile `shp` as I did, you can type `shp@` in your console and your RStudio's auto-complete functionality will show you that the slots in this object include `data`, `polygons`, `plotOrder`, `bbox`, and `proj4string`. It probably does not take much imagination to realize those correspond to some of the pieces of the shapefile I discussed previously.

```{r shpattribs}
shp@data %>% head()

shp@proj4string
```